# Any6D: Model-free 6D Pose Estimation of Novel Objects

This is the official implementation of our paper accepted by CVPR 2025 

[[Website]](https://sites.google.com/view/taeyeop-lee/any6d) [[Paper]](https://arxiv.org/pdf/2503.18673)

Authors: Taeyeop Lee, Bowen Wen, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon

<p align="center">
  <img src="./teaser/robot.gif" width="600"/>
</p>

# Abstract

We introduce Any6D, a model-free framework for 6D object pose estimation that requires only a single RGB-D anchor image to estimate both the 6D pose and size of unknown objects in novel scenes. Unlike existing methods that rely on textured 3D models or multiple viewpoints, Any6D leverages a joint object alignment process to enhance 2D-3D alignment and metric scale estimation for improved pose accuracy. Our approach integrates a render-and-compare strategy to generate and refine pose hypotheses, enabling robust performance in scenarios with occlusions, non-overlapping views, diverse lighting conditions, and large cross-environment variations. We evaluate our method on five challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O, demonstrating its effectiveness in significantly outperforming state-of-the-art methods for novel object pose estimation.


<p align="center">
  <img src="./teaser/teaser.png" width="800"/>
</p>


# Installation
Create `mamba` env. using [miniforge](https://github.com/conda-forge/miniforge).
```bash
#-- Create mamba env.
mamba create -n any6d python=3.12 
mamba activate any6d  
pip install uv 

#-- Install Pytorch 
uv pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128

#-- Eigen3 설치 for C++ modules in FoundationPose
mamba install conda-forge::eigen=3.4.0
mamba install -y conda-forge::boost

# -- CMAKE 경로 설정 (실제 경로 사용)
# ex) export CMAKE_PREFIX_PATH="$CMAKE_PREFIX_PATH:/eigen/path/under/conda"
export CMAKE_PREFIX_PATH="$CONDA_PREFIX/include:$CMAKE_PREFIX_PATH"

#-- install dependencies
uv pip install -r requirements.txt --index-strategy unsafe-best-match
```


```bash
#-- Install NVDiffRast (NVIDIA Differentiable Rasterizer)
uv pip install --no-build-isolation --no-cache-dir git+https://github.com/NVlabs/nvdiffrast.git

#-- Kaolin (PyTorch 2.7.0 + CUDA 12.8 버전) 
uv pip install --no-build-isolation --no-cache-dir kaolin==0.18.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.7.0_cu128.html
```

```bash
#-- PyTorch3D build from source (사전 빌드 wheel이 없으면 소스 빌드)
git clone https://github.com/facebookresearch/pytorch3d.git
cd pytorch3d
FORCE_CUDA=1 MAX_JOBS=4 uv pip install --no-build-isolation .
cd .. && rm -rf pytorch3d

#-- Build FoundationPose C++ extensions
# pybind11 CMake 경로를 자동으로 찾아서 설정
# 
CMAKE_PREFIX_PATH=$(python -c "import pybind11; print(pybind11.get_cmake_dir())") bash foundationpose/build_all_conda.sh
```

```bash
#-- build SAM2
cd sam2 && uv pip install -e . && cd checkpoints && \
./download_ckpts.sh && \
cd ..

#-- build InstantMesh (의존성만 설치)
cd instantmesh && uv pip install -r requirements.txt && cd ..
#-- build bop_toolkit
cd bop_toolkit && uv pip install -e . && cd .. 
```

### CheckPoints
Download the model checkpoints from the following 
- [foundationpose](https://drive.google.com/drive/folders/1DFezOAD0oD1BblsXVxqDsl8fj0qzB82i)
- [sam2](https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt)
- [instantmesh](https://huggingface.co/TencentARC/InstantMesh/tree/main)

Create the directory structure as follows:
```
foundationpose/
└── weights/
    ├── 2024-01-11-20-02-45/
    └── 2023-10-28-18-33-37/

sam2/
└── checkpoints/
    ├── sam2.1_hiera_large.pt
    
instantmesh/
└── ckpts/
    ├── diffusion_pytorch_model.bin
    └── instant_mesh_large.ckpt
```

### Download dataset
- [YCBV Models](https://drive.google.com/file/d/1gmcDD-5bkJfcMKLZb3zGgH_HUFbulQWu/view?usp=sharing)
- [HO3D Evaluation Files (evaluation.zip, masks_XMem.zip)](https://drive.google.com/drive/folders/1Wk-HZDvUExyUrRn7us4WWEbHnnFHgOAX)
- [Any6D Anchor Results](https://huggingface.co/datasets/taeyeop/Any6D/resolve/main/dexycb_reference_view_ours.zip?download=true)

[//]: # (- [YCBInEOAT]&#40;https://archive.cs.rutgers.edu/archive/a/2020/pracsys/Bowen/iros2020/YCBInEOAT/&#41;)
[//]: # (- [REAL275]&#40;https://github.com/hughw19/NOCS_CVPR2019&#41;)
[//]: # (- [Toyota-Light &#40;TOYL&#41;]&#40;https://bop.felk.cvut.cz/datasets/&#41;)
 

# Run Demo
```
python run_demo.py
python run_demo.py --img_to_3d # running instantmesh + sam2
``` 
# Run on Public Datasets (HO3D)
### Dataset Format
```
ho3d/
├── evaluation/         # HO3D evaluation files (e.g., annotations)
├── masks_XMem/         # Segmentation masks generated by XMem
└── YCB_Video_Models/   # 3D models for YCB objects (used in HO3D)
```
### 1. Run Anchor Image
We provided our input, image-to-3d results and anchor results [huggingface](https://huggingface.co/datasets/taeyeop/Any6D/resolve/main/dexycb_reference_view_ours.zip). 
```
python run_ho3d_anchor.py \
  --anchor_folder /anchor_results/dexycb_reference_view_ours \    # Path to anchor results
  --ycb_model_path /dataset/ho3d/YCB_Video_Models                 # Path to YCB models
  # --img_to_3d                                                   # Running instantmesh + sam2
```

### 2.  Run Query Image
```
python run_ho3d_query.py \
  --anchor_path /anchor_results/dexycb_reference_view_ours \     # Path to anchor results
  --hot3d_data_root /dataset/ho3d \                              # Root path to HO3D dataset
  --ycb_model_path /dataset/ho3d/YCB_Video_Models                # Path to YCB models
```


# Acknowledgement
We would like to acknowledge the contributions of public projects [FoundationPose](https://github.com/NVlabs/FoundationPose), [InstantMesh](https://github.com/TencentARC/InstantMesh), [SAM2](https://github.com/facebookresearch/sam2), [Oryon](https://github.com/jcorsetti/oryon) and [bop_toolkit](https://github.com/thodan/bop_toolkit) for their code release. We also thank the CVPR reviewers and Area Chair for their appreciation of this work and their constructive feedback.

# Citations
```
@inproceedings{lee2025any6d,
    title     = {{Any6D}: Model-free 6D Pose Estimation of Novel Objects},
    author    = {Lee, Taeyeop and Wen, Bowen and Kang, Minjun and Kang, Gyuree and Kweon, In So and Yoon, Kuk-Jin},
    booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},
    year      = {2025},
}
```























